{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_path = \"archive (2).zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables to hold the paths to the training and testing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to hold the paths to the training and testing CSV files\n",
    "training_csv_path = None\n",
    "testing_csv_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the uploaded file is a zip file\n",
    "if archive_path.endswith('.zip'):\n",
    "    with zipfile.ZipFile(archive_path) as zip_ref:\n",
    "        # Extract the names of all files in the zip archive\n",
    "        file_names = zip_ref.namelist()\n",
    "        \n",
    "        # Search for CSV files and separate them into training and testing files\n",
    "        for file_name in file_names:\n",
    "            if file_name.endswith('.csv'):\n",
    "                if 'train' in file_name.lower():\n",
    "                    training_csv_path = file_name\n",
    "                elif 'test' in file_name.lower():\n",
    "                    testing_csv_path = file_name\n",
    "        \n",
    "        # Check if both training and testing CSV files were found\n",
    "        if training_csv_path is None or testing_csv_path is None:\n",
    "            print(\"Error: Both training and testing CSV files not found in the zip archive.\")\n",
    "        else:\n",
    "            # Extract the training and testing CSV files\n",
    "            with zip_ref.open(training_csv_path) as train_file, zip_ref.open(testing_csv_path) as test_file:\n",
    "                # Read the CSV files into Pandas DataFrames\n",
    "                train_df = pd.read_csv(train_file)\n",
    "                test_df = pd.read_csv(test_file)\n",
    "                print(\"Data loaded successfully from the zip archive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of images in the respective classes 0 - Brain Tumor and 1 - healthy \n",
    "ROOT_DIR = \"BrainTumorData\"\n",
    "number_of_images = {}\n",
    "\n",
    "for dir in os.listdir(ROOT_DIR):\n",
    "    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Brain Tumor', 2418), ('Healthey', 1879)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_images.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"BrainTumorData\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we will split the data such that\n",
    "* 70% for Train Data\n",
    "* 15% for Validation\n",
    "* 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a trainning folder\n",
    "\n",
    "if not os.path.exists(\"./train\"):\n",
    "    os.mkdir(\"./train\")\n",
    "\n",
    "    for dir in os.listdir(ROOT_DIR):\n",
    "        os.makedirs(\"./train/\"+ dir)\n",
    "\n",
    "        for img in np.random.choice(a = os.listdir(os.path.join(ROOT_DIR,dir)), \n",
    "                                    size=(math.floor(70/100*number_of_images[dir]) - 5), \n",
    "                                    replace=False):\n",
    "            O = os.path.join(ROOT_DIR,dir,img) #path\n",
    "            D = os.path.join(\"./train\", dir)\n",
    "            shutil.copy(O,D)\n",
    "            os.remove(O)\n",
    "else:\n",
    "    print(\"The folder exsist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFolder(p, split):\n",
    "    if not os.path.exists(\"./\"+p):\n",
    "        os.mkdir(\"./\"+p)\n",
    "\n",
    "        for dir in os.listdir(ROOT_DIR):\n",
    "            os.makedirs(\"./\"+p+\"./\"+ dir)\n",
    "\n",
    "            for img in np.random.choice(a = os.listdir(os.path.join(ROOT_DIR,dir)), \n",
    "                                    size=(math.floor(split*number_of_images[dir]) - 5), \n",
    "                                    replace=False):\n",
    "                O = os.path.join(ROOT_DIR,dir,img) #path\n",
    "                D = os.path.join(\"./\"+p, dir)\n",
    "                shutil.copy(O,D)\n",
    "                os.remove(O)\n",
    "    else:\n",
    "        print(f\"{p} Folder exsist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Folder exsist\n"
     ]
    }
   ],
   "source": [
    "dataFolder(\"train\",0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Folder exsist\n"
     ]
    }
   ],
   "source": [
    "dataFolder(\"val\",0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Folder exsist\n"
     ]
    }
   ],
   "source": [
    "dataFolder(\"test\",0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Brain Tumor', 2418), ('Healthey', 1879)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of images in the respective classes 0 - Brain Tumor and 1 - healthy \n",
    "ROOT_DIR = \"BrainTumorData\"\n",
    "number_of_images = {}\n",
    "\n",
    "for dir in os.listdir(ROOT_DIR):\n",
    "    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)))\n",
    "\n",
    "number_of_images.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAvgPool2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 220, 220, 36)      5220      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 110, 110, 36)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 108, 108, 64)      20800     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                5537856   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5638245 (21.51 MB)\n",
      "Trainable params: 5638245 (21.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size=(3,3), activation='relu', input_shape = (224,224,3)))\n",
    "\n",
    "model.add(Conv2D(filters = 36, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(rate = 0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss = keras.losses.binary_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing our data using Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingImages(path):\n",
    "    '''\n",
    "    input : path\n",
    "    output : pre processed images\n",
    "    '''\n",
    "\n",
    "    image_data = ImageDataGenerator(zoom_range= 0.2, shear_range= 0.2, rescale= 1/255, horizontal_flip= True)\n",
    "    image = image_data.flow_from_directory(directory= path, target_size=(224,224), batch_size= 32, class_mode='binary')\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "path = \"train\"\n",
    "train_data = preprocessingImages(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "path = \"test\"\n",
    "test_data = preprocessingImages(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "path = \"val\"\n",
    "val_data = preprocessingImages(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#early stopping\n",
    "\n",
    "es = EarlyStopping(monitor = \"val_accuracy\", min_delta= 0.01, patience = 3, verbose= 1, mode= 'auto')\n",
    "\n",
    "#Model check point\n",
    "mc = ModelCheckpoint(monitor = \"val_accuracy\", filepath = \"./bestmodel.h5\", verbose =1, save_best_only = True, mode = \"auto\")\n",
    "\n",
    "cd = [es,mc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_19068\\1884689697.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hs = model.fit_generator(generator=train_data,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3/8 [==========>...................] - ETA: 3s - loss: 0.7363 - accuracy: 0.6104WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 240 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67308, saving model to .\\bestmodel.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 11s 731ms/step - loss: 0.7363 - accuracy: 0.6104 - val_loss: 0.6549 - val_accuracy: 0.6731\n"
     ]
    }
   ],
   "source": [
    "hs = model.fit_generator(generator=train_data,\n",
    "                         steps_per_epoch=8, \n",
    "                         epochs= 30, \n",
    "                         verbose = 1, \n",
    "                         validation_data= val_data , \n",
    "                         validation_steps= 16 , \n",
    "                         callbacks=cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
